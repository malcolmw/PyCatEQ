#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Wed Jan 11 13:14:45 2023

@author: malcolmw
"""
import argparse
import configparser
import h5py
import multiprocessing as mp
import numpy as np
import os
import pandas as pd
import pathlib
import sys
import tqdm

import my_logging

logger = my_logging.get_logger(__name__)

def main():
    clargs = parse_clargs()
    config = parse_config(clargs.config_file)
    my_logging.configure_logger(
        __name__,
        clargs.log_file,
        verbose=clargs.verbose
    )
    log_clargs(clargs)
    log_config(config)

    files = open_files(config['general']['dt_path'])
    event_pairs = get_event_pairs(files)
    unique_pairs = get_unique_event_pairs(event_pairs)
    close_files(files)

    initargs = (config, event_pairs)

    with mp.Pool(clargs.n_proc, initargs=initargs, initializer=target_init) as pool:
        pool.map(target, np.array_split(unique_pairs, clargs.n_proc))
    
def parse_clargs():
    '''
    Parse and return command line arguments.
    '''

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-c',
        '--config_file',
        type=str,
        default=f'{sys.argv[0]}.cfg',
        help='Configuration file.'
    )
    parser.add_argument(
        '-l',
        '--log_file',
        type=str,
        default=f'{sys.argv[0]}.log',
        help='Log file.'
    )
    parser.add_argument(
        '-n',
        '--n_proc',
        type=int,
        default=1,
        help='Number of processors.'
    )
    parser.add_argument(
        '-v',
        '--verbose',
        action='store_true',
        help='Be verbose.'
    )
    clargs = parser.parse_args()

    return clargs


def parse_config(config_file):
    config = dict()
    parser = configparser.ConfigParser()
    parser.read(config_file)
    config['general'] = dict(
        cc_thresh=parser.getfloat('general', 'cc_thresh'),
        dtt_max=parser.getfloat('general', 'dtt_max'),
        ncc_min=parser.getint('general', 'ncc_min'),
        dt_path=parser.get('general', 'dt_path'),
        catalog_path=parser.get('general', 'catalog_path'),
        output_path=parser.get('general', 'output_path'),
    )

    return config


def log_clargs(clargs):
    name = pathlib.Path(sys.argv[0]).name
    logger.info(f'Starting script {name}...')
    logger.info('')
    logger.info('***Command-line arguments***')
    for arg in sorted(vars(clargs)):
        logger.info(f'{arg}: {getattr(clargs, arg)}')
    logger.info('')
    
    
def log_config(config):
    logger.info('')
    logger.info('***Configuration***')
    for group in sorted(config):
        if not isinstance(config[group], dict):
            value = config[group]
            logger.info(f'{group}: {value}')
            continue
        for key in sorted(config[group]):
            value = config[group][key]
            logger.info(f'{group}.{key}: {value}')
    logger.info('')


def target_init(_config, _event_pairs):
    global config
    global events, arrivals, files
    global event_pairs
    global output_path
   
    config = _config
    event_pairs = _event_pairs
    
    filename = f'dt-{os.getpid()}.cc' 
    output_path = pathlib.Path(config['general']['output_path']).joinpath(filename)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    events = pd.read_hdf(config['general']['catalog_path'], key='/events')
    events = events.set_index('event_id')
    events = events.sort_index()

    arrivals = pd.read_hdf(config['general']['catalog_path'], key='/arrivals')
    arrivals = arrivals.set_index(['event_id', 'network', 'station', 'phase'])
    arrivals = arrivals.sort_index()
    
    files = open_files(config['general']['dt_path'])


def target(unique_pairs):
    global output_path
    global config
    
    with open(output_path, mode='w', buffering=1024) as out_file:
        for event_id_A, event_id_B in tqdm.tqdm(unique_pairs):
            dataf = get_measurements(event_id_A, event_id_B)
            dataf = dataf[
                 (np.abs(dataf['cc']) > config['general']['cc_thresh'])
                &(np.abs(dataf['dt']) < config['general']['dtt_max'])
            ]

            if len(dataf) < config['general']['ncc_min']:
                continue

            blob = f'# {event_id_A} {event_id_B} 0.0\n'
            blob += '\n'.join((
                f'{network:2s} {station:5s} {phase} {dt:6.3f} {cc:6.3f}' 
                for _, (network, station, phase, dt, cc) in dataf.iterrows()
            ))
            out_file.write(blob + '\n')


def open_files(path):
    files = dict()
    for path in sorted(pathlib.Path(path).iterdir()):
        network, station, _ = path.name.split('.')
        files[(network, station)] = h5py.File(path, mode='r')
        
    return files

def close_files(files):
    for key in files:
        files[key].close()
        
    
def get_event_pairs(files):
    event_pairs = dict()
    for key in files:
        for phase in ('P', 'S'):
            event_pairs[(*key, phase)] = files[key][f'/event_pair_ids/{phase}'][:]
    return event_pairs


def get_unique_event_pairs(event_pairs):
    event_pairs = np.vstack([
        event_pairs[key]
        for key in event_pairs
    ])
    event_pairs = np.sort(event_pairs, axis=1)
    unique_pairs = pd.Series(list(zip(event_pairs[:, 0], event_pairs[:, 1]))).unique()
    
    return unique_pairs


def get_measurements(event_id_A, event_id_B):
    global event_pairs, arrivals, files
    
    measurements = list()
    for key in event_pairs:
        phase = key[2]
        idx = np.argwhere(
             (event_pairs[key][:, 0] == event_id_A)
            &(event_pairs[key][:, 1] == event_id_B)
        ).flatten()
        if len(idx) == 1:
            # print(key, idx)
            dt = files[key[:2]][f'/dt/{phase}'][idx[0]]
            cc = files[key[:2]][f'/cc_max/{phase}'][idx[0]]
            measurements.append([*key, dt, cc])
        idx = np.argwhere(
             (event_pairs[key][:, 0] == event_id_B)
            &(event_pairs[key][:, 1] == event_id_A)
        ).flatten()
        if len(idx) == 1:
            # print(key, idx)
            dt = -files[key[:2]][f'/dt/{phase}'][idx[0]]
            cc = files[key[:2]][f'/cc_max/{phase}'][idx[0]]
            measurements.append([*key, dt, cc])
            
    dataf = pd.DataFrame(measurements, columns=['network', 'station', 'phase', 'dt', 'cc'])
    dataf = dataf.groupby(['network', 'station', 'phase']).mean()
    
    arrivals_A = arrivals.loc[event_id_A].loc[dataf.index]
    arrivals_B = arrivals.loc[event_id_B].loc[dataf.index]
    dataf['dt'] = dataf['dt'] - (arrivals_B['time'] - arrivals_A['time']).dt.total_seconds()
    dataf = dataf.reset_index()
    
    return dataf

    
if __name__ == '__main__':
    main()
