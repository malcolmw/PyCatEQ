#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Wed Jan 11 13:14:45 2023

@author: malcolmw

- event_pair_ids will be in (template_event_id, test_event_id) order.
"""
import argparse
import configparser
import h5py
import numpy as np
import pandas as pd
import pathlib
import pyproj
import sys
import tqdm

global ndimage
global spx
global xp

try:
    import cupy as cp
    import cupyx.scipy
    import cupyx.scipy.ndimage
    spx = cupyx.scipy
    ndimage = cupyx.scipy.ndimage
    get_array_module = cp.get_array_module
    asnumpy = cp.asnumpy
    DEVICE_COUNT = cp.cuda.runtime.getDeviceCount()
except ModuleNotFoundError:
    DEVICE_COUNT = 0

if DEVICE_COUNT < 1:
    get_array_module = lambda array: np
    import scipy
    import scipy.ndimage
    spx = scipy
    ndimage = scipy.ndimage
    asnumpy = lambda array: array

import my_logging

logger = my_logging.get_logger(__name__)

def main():
    global xp
    global spx
    global ndimage

    clargs = parse_clargs()
    ##############
    # SPYDER START
    ##############
    clargs.config_file='/home/malcolmw/git/PyCatEQ/cfg/410_measure_differentials.cfg'
    clargs.network = 'AZ'
    clargs.station = 'BZN'
    ##############
    # SPYDER END
    ##############
    config = parse_config(clargs.config_file)
    ##############
    # SPYDER START
    ##############
    for path in config['general']['output_dir'].iterdir():
        path.unlink()
    ##############
    # SPYDER END
    ##############
    my_logging.configure_logger(
        __name__,
        clargs.log_file,
        verbose=clargs.verbose
    )

    # Choose the appropriate array module (NumPy/CuPy).
    if clargs.gpu is False:
        xp = np
        import scipy
        import scipy.ndimage
        spx = scipy
        ndimage = scipy.ndimage
    elif DEVICE_COUNT > 0: # GPU requested and is available
        xp = cp

    log_clargs(clargs)
    log_config(config)

    if (clargs.gpu is False or DEVICE_COUNT < 1) and clargs.n_proc > 1:
        main_parallel(config, clargs)
    else:
        main_serial(config, clargs)


def main_parallel(config, clargs):
    import multiprocessing as mp
    logger.info(f'Processing data in parallel using {clargs.n_proc} CPUs.')
    wf_file = config['general']['input_dir'].joinpath(
        f'{clargs.network}.{clargs.station}.hdf5'
    )
    logger.info(wf_file)

    # Load the event database
    events = load_events(config)
    out_file = initialize_output(config, clargs, events.index)
    id_map = pd.Series(np.arange(len(events)), index=events.index)

    with h5py.File(out_file, mode='a') as out_file:
        for phase in clargs.phases:
            logger.info(f'Cross-correlating {phase}-wave arrivals.')
            # Load the event database
            events = load_events(config)
            # Discard events without waveforms
            with h5py.File(wf_file, mode='r') as _wf_file:
                events = select_events_with_waveforms(events, _wf_file, phase)

            with mp.Pool(
                clargs.n_proc,
                initializer=init_correlate_event_parallel,
                initargs=(events, config, wf_file)
            ) as pool:
                for results in tqdm.tqdm(
                    pool.imap(
                        correlate_event_parallel,
                        ((phase, event) for event_id, event in events.iterrows())
                    ),
                    total=len(events)
                ):
                    write_results(results, phase, out_file, id_map)
   
def main_serial(config, clargs):
    logger.info('Processing data serially.')
    wf_file = config['general']['input_dir'].joinpath(
        f'{clargs.network}.{clargs.station}.hdf5'
    )
    logger.info(wf_file)

    # Load the event database
    events = load_events(config)
    out_file = initialize_output(config, clargs, events.index)
    
    with (
            h5py.File(out_file, mode='a') as out_file, 
            h5py.File(wf_file, mode='r') as wf_file
    ):
        event_ids = wf_file['event_id'][:].astype(str)
        id_map = pd.Series(np.arange(len(event_ids)), index=event_ids)
        for phase in clargs.phases:
            logger.info(f'Cross-correlating {phase}-wave arrivals.')
            # Load the event database
            events = load_events(config)
            # Discard events without waveforms
            events = select_events_with_waveforms(events, wf_file, phase)
            
            for event_id, event in tqdm.tqdm(events.iterrows(), total=len(events)):
                results = correlate_event_serial(
                    phase,
                    event, 
                    events, 
                    config, 
                    wf_file,
                )
                write_results(results, phase, out_file, id_map)

def write_results(results, phase, out_file, id_map):
    n_obs = len(results['event_pair_ids'])
    datas = out_file[f'/event_pair_ids/{phase}']
    size = len(datas)
    datas.resize(size+n_obs, axis=0)
    datas[size: ] = results['event_pair_ids']
    for i_result, event_pair in enumerate(results['event_pair_ids']):
        event_id_A, event_id_B = min(event_pair), max(event_pair)
        serial_id_A, serial_id_B = id_map[event_id_A], id_map[event_id_B]
        dt = results['dt'][i_result]
        cc = results['cc_max'][i_result]
        out_file[f'/dt/{phase}'][serial_id_A, serial_id_B] = dt
        out_file[f'/cc_max/{phase}'][serial_id_A, serial_id_B] = cc
                
    
def parse_clargs():
    '''
    Parse and return command line arguments.
    '''

    parser = argparse.ArgumentParser()
    # parser.add_argument(
    #     'network',
    #     type=str,
    #     help='Network code.'
    # )
    # parser.add_argument(
    #     'station',
    #     type=str,
    #     help='Station code.'
    # )
    parser.add_argument(
        '-c',
        '--config_file',
        type=str,
        default=f'{sys.argv[0]}.cfg',
        help='Configuration file.'
    )
    parser.add_argument(
        '-d',
        '--device_number',
        type=int,
        default=0,
        help='CUDA device number to run on. This value is ignored unless the '
        '-g option is specified.'
    )
    parser.add_argument(
        '-g',
        '--gpu',
        action='store_true',
        help='Run on GPU.'
    )
    parser.add_argument(
        '-l',
        '--log_file',
        type=str,
        default=f'{sys.argv[0]}.log',
        help='Log file.'
    )
    parser.add_argument(
        '-n',
        '--n_proc',
        type=int,
        default=1,
        help='Number of parallel processes to run. This value is ignored if '
        'the -d option is specified.'
    )
    parser.add_argument(
        '-p',
        '--phases',
        type=str,
        default=f'PS',
        help='Phase(s) to process.'
    )
    parser.add_argument(
        '-v',
        '--verbose',
        action='store_true',
        help='Be verbose.'
    )
    clargs = parser.parse_args()

    if clargs.gpu is True and DEVICE_COUNT == 0:
        logger.warning('GPU was requested but is unavailable. Running on CPU(s).')

    return clargs


def parse_config(config_file):
    config = dict()
    parser = configparser.ConfigParser()
    parser.read(config_file)
    config['general'] = dict(
        input_dir=pathlib.Path(parser.get('general', 'input_dir')).resolve(),
        output_dir=pathlib.Path(parser.get('general', 'output_dir')).resolve(),
        catalog=pathlib.Path(parser.get('general', 'catalog')).resolve(),
        max_dist=parser.getfloat('general', 'max_dist', fallback=None),
        n_pairs=parser.getint('general', 'n_pairs', fallback=100)
    )
    config['waveforms'] = dict(
        template_tlead_P=parser.getfloat('waveforms', 'template_tlead_P'),
        template_tlag_P=parser.getfloat('waveforms', 'template_tlag_P'),
        template_tlead_S=parser.getfloat('waveforms', 'template_tlead_S'),
        template_tlag_S=parser.getfloat('waveforms', 'template_tlag_S')
    )
    config['correlation'] = dict(
        threshold=parser.getfloat('correlation', 'threshold'),
        absolute=parser.getboolean('correlation', 'absolute')
    )

    return config


def init_correlate_event_parallel(_events, _config, _wf_file):
    global events
    global config
    global wf_file
    events = _events.copy()
    config = _config
    wf_file = h5py.File(_wf_file, mode='r')


def correlate_event_parallel(args):
    global events
    global config
    global wf_file
    phase, event = args
    n_pairs = config['general']['n_pairs']

    # Sort events by distance from template event
    sorted_events = dist_sort(
        events, 
        event, 
        max_dist=config['general']['max_dist']
    )
    
    selected_events, selected_pair_ids = select_events(
        event,
        sorted_events,
        n_pairs,
    )
    selected_events = selected_events.sort_index()
    
    # Get the waveforms
    X_template, X_test, template_start_time, test_start_times = get_waveforms(
        wf_file,
        phase,
        event,
        selected_events,
        config
    )
    
    # DO the correlation
    # For now, just run on GPU.
    X_template = xp.array(X_template)
    X_test = xp.array(X_test)
    test_start_times = xp.array(test_start_times)
    template_start_time = xp.array(template_start_time)
    xcorr = correlate_wfs(X_template, X_test)

    if config['correlation']['absolute'] is True:
        lag = xp.argmax(xp.abs(xcorr), axis=1)
    else:
        lag = xp.argmax(xcorr, axis=1)
    i0 = np.repeat(np.arange(len(lag)), 3)
    i1 = lag.flatten()
    i2 = np.tile(np.arange(3), len(lag))
    cc_max = xcorr[i0, i1, i2].reshape(-1, 3)

    n_samp = X_template.shape[0]
    lag -= n_samp//2 + n_samp%2

    sampling_rate = wf_file[phase].attrs['sampling_rate']
    #tlead = wf_file[phase].attrs['n_samp_lead'] / sampling_rate
    #template_tlead = config['waveforms'][f'template_tlead_{phase}']
    dt  = (test_start_times - template_start_time[xp.newaxis])
    dt += lag / sampling_rate
    #dt += (tlead - template_tlead) - lag / sampling_rate
    dt  = xp.sum(dt * xp.abs(cc_max), axis=-1)
    dt /= xp.sum(xp.abs(cc_max), axis=-1)
    cc_max = xp.mean(cc_max, axis=-1)

    event_pair_ids = form_pairs(
        event.name, 
        selected_events.index, 
        sort=False
    )
    event_pair_ids = np.asarray(list(event_pair_ids))

    # Discard measurements below threshold.
    thresh = config['correlation']['threshold']
    absolute = config['correlation']['absolute']
    dt = asnumpy(dt)
    cc_max = asnumpy(cc_max)
    if absolute is True:
        idxs = np.nonzero(np.abs(cc_max) >= thresh)
    else:
        idxs = np.nonzero(cc_max >= thresh)
    dt = dt[idxs]
    cc_max = cc_max[idxs]
    event_pair_ids = event_pair_ids[idxs]

    return dict(
        dt=dt, 
        cc_max=cc_max,
        event_pair_ids=event_pair_ids,
        sorted_pair_ids=selected_pair_ids
    )

def plot(X_template, X_test):
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots()
    ax.plot(X_test[-1]);
    plt.show()
    

def correlate_event_serial(phase, event, events, config, wf_file):
    n_pairs = config['general']['n_pairs']
    
    # Only consider events that occurred earlier in time.
    events = events[events['time'] < event['time']].copy()

    # Sort events by distance from template event
    sorted_events = dist_sort(
        events, 
        event, 
        max_dist=config['general']['max_dist']
    )
    
    selected_events, selected_pair_ids = select_events(
        event,
        sorted_events,
        n_pairs,
    )
    selected_events = selected_events.sort_index()
    
    
    # Get the waveforms
    X_template, X_test, template_start_time, test_start_times = get_waveforms(
        wf_file,
        phase,
        event,
        selected_events,
        config
    )
    
    # DO the correlation
    # For now, just run on GPU.
    X_template = xp.array(X_template)
    X_test = xp.array(X_test)
    test_start_times = xp.array(test_start_times)
    template_start_time = xp.array(template_start_time)
    xcorr = correlate_wfs(X_template, X_test)

    if config['correlation']['absolute'] is True:
        lag = xp.argmax(xp.abs(xcorr), axis=1)
    else:
        lag = xp.argmax(xcorr, axis=1)
    i0 = np.repeat(np.arange(len(lag)), 3)
    i1 = lag.flatten()
    i2 = np.tile(np.arange(3), len(lag))
    cc_max = xcorr[i0, i1, i2].reshape(-1, 3)

    n_samp = X_template.shape[0]
    lag -= n_samp//2 + n_samp%2

    sampling_rate = wf_file[phase].attrs['sampling_rate']
    dt  = (test_start_times - template_start_time[xp.newaxis])
    dt += lag / sampling_rate
    dt  = xp.sum(dt * xp.abs(cc_max), axis=-1)
    dt /= xp.sum(xp.abs(cc_max), axis=-1)
    cc_max = xp.mean(cc_max, axis=-1)

    event_pair_ids = form_pairs(
        event.name, 
        selected_events.index, 
        sort=False
    )
    event_pair_ids = np.asarray(list(event_pair_ids))

    # Discard measurements below threshold.
    thresh = config['correlation']['threshold']
    absolute = config['correlation']['absolute']
    dt = asnumpy(dt)
    cc_max = asnumpy(cc_max)
    if absolute is True:
        idxs = np.nonzero(np.abs(cc_max) >= thresh)
    else:
        idxs = np.nonzero(cc_max >= thresh)
    dt = dt[idxs]
    cc_max = cc_max[idxs]
    event_pair_ids = event_pair_ids[idxs]

    return dict(
        dt=dt, 
        cc_max=cc_max,
        event_pair_ids=event_pair_ids,
        sorted_pair_ids=selected_pair_ids
    )


def correlate_wfs(X_template, X_test):
    xp = get_array_module(X_template)

    X_template = normalize(X_template, axis=0)
    X_test = normalize(X_test, axis=1)
    
    n_samp = X_template.shape[0]

    xcorr = xp.dstack([
        spx.ndimage.correlate1d(
            X_test[..., i_chan], 
            X_template[..., i_chan],
            mode='constant'
        )
        for i_chan in range(3)
    ])
    X_test = sliding_window(X_test, n_samp, axis=1)
    norm = (
        xp.sqrt(xp.sum(xp.square(X_template), axis=0))
        * 
        xp.sqrt(xp.sum(xp.square(X_test), axis=2))
    )
    
    return xp.nan_to_num(xcorr / norm)

    
def normalize(X, axis=0):
    xp = get_array_module(X)
    mu = xp.mean(X, axis=axis)
    inds = [slice(None) for i in range(X.ndim)]
    inds[axis] = xp.newaxis
    inds = tuple(inds)
    X = X - mu[inds]
    sigma = xp.std(X, axis=axis)
    X = X / sigma[inds]
    X = xp.nan_to_num(X)
    
    return X

    
def select_events(event, sorted_events, n_pairs):
    
    # Form a list of event pair IDs for the sorted events.
    #pairs = form_pairs(event.name, sorted_events.index.values)

    # TODO:: Retain only the sorted events that have not already been 
    # correlated against the template event.

    # Retain only the n_pairs nearest neighbours.
    selected_events = sorted_events.iloc[:n_pairs]

    # Remember the event pair IDs that were just correlated.
    selected_pairs = form_pairs(event.name, selected_events.index.values)
    
    return selected_events, selected_pairs


def sliding_window(X, window_length, axis=0):
    xp = get_array_module(X)
    n_pad = window_length // 2
    odd_bit = window_length % 2
    pad = [(0, 0)] * X.ndim
    pad[axis] = (n_pad+odd_bit, n_pad)
    X = xp.pad(X, pad)
    n_windows = X.shape[axis] - window_length
    shape = (*X.shape[:axis], n_windows, window_length, *X.shape[axis+1:])
    strides = (
        *X.strides[:axis],
         X.strides[axis],
        *X.strides[axis:]
    )
    X_new = xp.lib.stride_tricks.as_strided(
        X.copy(),
        shape=shape,
        strides=strides,
    )

    return X_new


def dist_sort(events, event, max_dist=None):

    # events = events.drop(index=event.name)
    
    geod = pyproj.Geod(ellps='WGS84')
    az, baz, dist = geod.inv(
        np.repeat(event['longitude'], len(events)),
        np.repeat(event['latitude'], len(events)),
        events['longitude'],
        events['latitude']
    )
    events['dist'] = np.sqrt(
        np.square(dist * 1e-3)
        +np.square(events['depth'] - event['depth'])
    )

    if max_dist is not None:
        events = events[events['dist'] < max_dist]

    events = events.sort_values('dist')
    
    return events


def form_pairs(template_event_id, test_event_ids, sort=True):
    # Form a list of event pair IDs for the sorted events.
    pairs = np.stack([
        np.repeat(template_event_id, len(test_event_ids)), 
        test_event_ids
    ])

    if sort is True:
        # Order event pair IDs
        pairs = np.stack([
            np.min(pairs, axis=0), 
            np.max(pairs, axis=0)
        ])

    # Create a tuple of event pair IDs to access pd.Series.isin() method
    pairs = pd.Series(tuple(zip(pairs[0], pairs[1])), dtype=object)

    return pairs


def select_events_with_waveforms(events, wf_file, phase):
    event_ids = wf_file['event_id'][:].astype(str)
    array_index_map = pd.Series(
        np.arange(len(event_ids)),
        index=event_ids
    )
    mask = wf_file[f'mask_{phase}'][:]
    event_ids = event_ids[mask]
    # Just events with waveforms
    events = events.loc[event_ids]
    # Assign lookup index.
    events['array_index'] = array_index_map[event_ids]
    
    return events


def get_waveforms(wf_file, phase, template_event, test_events, config):
    n_samp_lead = wf_file[phase].attrs['n_samp_lead']
    n_samp_lag = wf_file[phase].attrs['n_samp_lag']
    sampling_rate = wf_file[phase].attrs['sampling_rate']
    
    template_tlead = config['waveforms'][f'template_tlead_{phase}']
    template_tlag  = config['waveforms'][f'template_tlag_{phase}']
    template_tlead = config['waveforms'][f'template_tlead_{phase}']
    template_n_samp_lead = int(sampling_rate*template_tlead)
    template_n_samp_lag = int(sampling_rate*template_tlag)
    
    i_samp_start = n_samp_lead - template_n_samp_lead
    i_samp_end   = n_samp_lead + template_n_samp_lag
    if i_samp_start < 0 or i_samp_end > n_samp_lead+n_samp_lag:
        logger.warning('Event waveforms are too short for request template length')
        i_samp_start = max(0, i_samp_start)
        i_samp_end = min(n_samp_lead+n_samp_lag, i_samp_end)
    
    template_event_idx = template_event['array_index']
    test_event_idxs = test_events['array_index'].values
    
    template_wf = wf_file[phase][template_event_idx, i_samp_start: i_samp_end]
    template_start_time = wf_file[f'start_time_{phase}'][template_event_idx]
    template_start_time += i_samp_start / sampling_rate

    test_wfs = wf_file[phase][test_event_idxs]
    test_start_times = wf_file[f'start_time_{phase}'][test_event_idxs]
    
    return template_wf, test_wfs, template_start_time, test_start_times


def initialize_output(config, clargs, event_ids):
    out_file = config['general']['output_dir'].joinpath(
        f'{clargs.network}.{clargs.station}.hdf5'
    )
    out_file.parent.mkdir(exist_ok=True, parents=True)
    n_events = len(event_ids)
    with h5py.File(out_file, mode='a') as out_file:
        out_file.create_dataset(
            f'/event_ids',
            data=event_ids,
            dtype=h5py.string_dtype()
        )
        for phase in clargs.phases:
            out_file.create_dataset(
                f'/event_pair_ids/{phase}',
                (0, 2),
                maxshape=(None, 2),
                dtype=h5py.string_dtype()
            )
            out_file.create_dataset(
                f'/dt/{phase}',
                (n_events, n_events),
                maxshape=(None, None),
                dtype=np.float64,
                chunks=(32, 32),
                compression=3
            )
            out_file.create_dataset(
                f'/cc_max/{phase}',
                (n_events, n_events),
                maxshape=(None, None),
                dtype=np.float32,
                chunks=(32, 32),
                compression=3
            )
        file_name = out_file.filename

    return file_name
    

def load_events(config):
    events = pd.read_hdf(config['general']['catalog'], key='/events')
    events = events.set_index('event_id')
    events = events.sort_index()

    return events


def log_clargs(clargs):
    name = pathlib.Path(sys.argv[0]).name
    logger.info(f'Starting script {name}...')
    logger.info('')
    logger.info('***Command-line arguments***')
    for arg in sorted(vars(clargs)):
        logger.info(f'{arg}: {getattr(clargs, arg)}')
    logger.info('')
    
    
def log_config(config):
    logger.info('')
    logger.info('***Configuration***')
    for group in sorted(config):
        if not isinstance(config[group], dict):
            value = config[group]
            logger.info(f'{group}: {value}')
            continue
        for key in sorted(config[group]):
            value = config[group][key]
            logger.info(f'{group}.{key}: {value}')
    logger.info('')

    
if __name__ == '__main__':
    main()
