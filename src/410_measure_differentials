#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Wed Jan 11 13:14:45 2023

@author: malcolmw

- event_pairs will be in (template_event_id, test_event_id) order.
"""
import argparse
import configparser
import h5py
import multiprocessing as mp
import numpy as np
import pandas as pd
import pathlib
import pyproj
import sys
import tqdm

global ndimage
global spx
global xp

try:
    import cupy as cp
    import cupyx.scipy
    import cupyx.scipy.ndimage
    spx = cupyx.scipy
    ndimage = cupyx.scipy.ndimage
    get_array_module = cp.get_array_module
    asnumpy = cp.asnumpy
    DEVICE_COUNT = cp.cuda.runtime.getDeviceCount()
except (ModuleNotFoundError, ImportError):
    DEVICE_COUNT = 0

if DEVICE_COUNT < 1:
    get_array_module = lambda array: np
    import scipy
    import scipy.ndimage
    spx = scipy
    ndimage = scipy.ndimage
    asnumpy = lambda array: array

import my_logging

logger = my_logging.get_logger(__name__)

def main():
    global xp
    global spx
    global ndimage

    clargs = parse_clargs()
    my_logging.configure_logger(
        __name__,
        clargs.log_file,
        verbose=clargs.verbose
    )

    # Choose the appropriate array module (NumPy/CuPy).
    if clargs.gpu is False:
        xp = np
        import scipy
        import scipy.ndimage
        spx = scipy
        ndimage = scipy.ndimage
    elif DEVICE_COUNT > 0: # GPU requested and is available
        xp = cp

    log_clargs(clargs)
    log_config(config)

    wf_path = config['general']['input_dir'].joinpath(
        f'{clargs.network}.{clargs.station}.hdf5'
    )

    # Load the event database
    events = load_events(config)
    out_file = initialize_output(config, clargs, events.index)
    
    with (
            h5py.File(out_file, mode='a') as out_file, 
            h5py.File(wf_path, mode='r') as wf_file
    ):
        event_ids = wf_file['event_id'][:].astype(str)
        serial_id_map = pd.Series(np.arange(len(event_ids)), index=event_ids)
        for phase in clargs.phases:
            logger.info(f'Cross-correlating {phase}-wave arrivals.')
            # Load the event database
            events = load_events(config)
            # Discard events without waveforms
            events = select_events_with_waveforms(events, wf_file, phase)

            if (clargs.gpu is False or DEVICE_COUNT < 1) and clargs.n_proc > 1:
                with mp.Pool(
                    clargs.n_proc,
                    initializer=init_mp,
                    initargs=(events, config, wf_path)
                ) as pool:
                    generator = (
                        (phase, event) 
                        for event_id, event in events.iterrows()
                    )
                    for results in tqdm.tqdm(
                        pool.imap(correlate_event, generator),
                        total=len(events)
                    ):
                        write_results(results, phase, out_file, serial_id_map)
            else:
                for event_id, event in tqdm.tqdm(
                    events.iterrows(), 
                    total=len(events)
                ):
                    results = correlate_event((
                        phase,
                        event, 
                        events, 
                        config, 
                        wf_file,
                    ))
                    write_results(results, phase, out_file, serial_id_map)


def write_results(results, phase, out_file, serial_id_map):
    n_obs = len(results['event_pairs'])
    datas = out_file[f'/event_pairs/{phase}']
    size = len(datas)
    datas.resize(size+n_obs, axis=0)
    datas[size: ] = results['event_pairs']
    for i_result, event_pair in enumerate(results['event_pairs']):
        event_id_A, event_id_B = event_pair
        serial_id_A = serial_id_map[event_id_A]
        serial_id_B = serial_id_map[event_id_B]
        dt = results['dt'][i_result]
        cc = results['cc_max'][i_result]
        out_file[f'/dt/{phase}'][serial_id_A, serial_id_B] = dt
        out_file[f'/cc_max/{phase}'][serial_id_A, serial_id_B] = cc
                
    
def parse_clargs():
    '''
    Parse and return command line arguments.
    '''

    parser = argparse.ArgumentParser()
    parser.add_argument(
        'network',
        type=str,
        help='Network code.'
    )
    parser.add_argument(
        'station',
        type=str,
        help='Station code.'
    )
    parser.add_argument(
        '-c',
        '--config_file',
        type=str,
        default=f'{sys.argv[0]}.cfg',
        help='Configuration file.'
    )
    parser.add_argument(
        '-d',
        '--device_number',
        type=int,
        default=0,
        help='CUDA device number to run on. This value is ignored unless the '
        '-g option is specified.'
    )
    parser.add_argument(
        '-g',
        '--gpu',
        action='store_true',
        help='Run on GPU.'
    )
    parser.add_argument(
        '-l',
        '--log_file',
        type=str,
        default=f'{sys.argv[0]}.log',
        help='Log file.'
    )
    parser.add_argument(
        '-n',
        '--n_proc',
        type=int,
        default=1,
        help='Number of parallel processes to run. This value is ignored if '
        'the -d option is specified.'
    )
    parser.add_argument(
        '-p',
        '--phases',
        type=str,
        default='PS',
        help='Phase(s) to process.'
    )
    parser.add_argument(
        '-v',
        '--verbose',
        action='store_true',
        help='Be verbose.'
    )
    clargs = parser.parse_args()

    if clargs.gpu is True and DEVICE_COUNT == 0:
        logger.warning('GPU was requested but is unavailable. Running on CPU(s).')

    return clargs


def parse_config(config_file):
    config = dict()
    parser = configparser.ConfigParser()
    parser.read(config_file)
    config['general'] = dict(
        input_dir=pathlib.Path(parser.get('general', 'input_dir')).resolve(),
        output_dir=pathlib.Path(parser.get('general', 'output_dir')).resolve(),
        catalog=pathlib.Path(parser.get('general', 'catalog')).resolve(),
        max_dist=parser.getfloat('general', 'max_dist', fallback=None),
        n_pairs=parser.getint('general', 'n_pairs', fallback=100)
    )
    config['waveforms'] = dict(
        template_tlead_P=parser.getfloat('waveforms', 'template_tlead_P'),
        template_tlag_P=parser.getfloat('waveforms', 'template_tlag_P'),
        template_tlead_S=parser.getfloat('waveforms', 'template_tlead_S'),
        template_tlag_S=parser.getfloat('waveforms', 'template_tlag_S')
    )
    config['correlation'] = dict(
        threshold=parser.getfloat('correlation', 'threshold'),
        absolute=parser.getboolean('correlation', 'absolute')
    )

    return config


def init_mp(_events, _config, _wf_file):
    global events
    global config
    global wf_file

    events = _events.copy()
    config = _config
    wf_file = h5py.File(_wf_file, mode='r')


def correlate_event(args):
    if len(args) == 2:
        # Running in parallel mode, so use global variables.
        global events
        global config
        global wf_file
        phase, event = args
    else:
        # Running in serial
        phase, event, events, config, wf_file = args

    n_pairs = config['general']['n_pairs']
    
    # Only consider events that occurred earlier in time.
    events = events[events['time'] < event['time']].copy()

    # Sort events by distance from template event
    sorted_events = dist_sort(
        events, 
        event, 
        max_dist=config['general']['max_dist']
    )
    
    selected_events, selected_pair_ids = select_events(
        event,
        sorted_events,
        n_pairs,
    )
    selected_events = selected_events.sort_index()
    
    
    # Get the waveforms
    X_template, X_test, template_start_time, test_start_times = get_waveforms(
        wf_file,
        phase,
        event,
        selected_events,
        config
    )
    
    # DO the correlation
    X_template = xp.array(X_template)
    X_test = xp.array(X_test)
    test_start_times = xp.array(test_start_times)
    template_start_time = xp.array(template_start_time)
    xcorr = correlate_wfs(X_template, X_test)

    if config['correlation']['absolute'] is True:
        lag = xp.argmax(xp.abs(xcorr), axis=1)
    else:
        lag = xp.argmax(xcorr, axis=1)
    # Extract the maximum xcorr value along the time axis for each trace
    i0 = np.repeat(np.arange(len(lag)), 3)
    i1 = lag.flatten()
    i2 = np.tile(np.arange(3), len(lag))
    cc_max = xcorr[i0, i1, i2].reshape(-1, 3)
    
    # Extract the maximum xcorr value along the channel axis for each
    # event.
    if config['correlation']['absolute'] is True:
        i1 = xp.argmax(xp.abs(cc_max), axis=-1)
    else:
        i1 = xp.argmax(cc_max, axis=-1)
    i0 = np.arange(len(i1))
    cc_max = cc_max[i0, i1]
    lag = lag[i0, i1]
    
    n_samp = X_template.shape[0]
    lag -= n_samp//2 + n_samp%2

    sampling_rate = wf_file[phase].attrs['sampling_rate']
    dt  = test_start_times[i0, i1] - template_start_time[i1]
    dt += lag / sampling_rate

    event_pairs = form_pairs(
        event.name, 
        selected_events.index, 
        sort=False
    )
    event_pairs = np.asarray(list(event_pairs))

    # Discard measurements below threshold.
    thresh = config['correlation']['threshold']
    absolute = config['correlation']['absolute']
    dt = asnumpy(dt)
    cc_max = asnumpy(cc_max)
    if absolute is True:
        idxs = np.nonzero(np.abs(cc_max) >= thresh)
    else:
        idxs = np.nonzero(cc_max >= thresh)
    dt = dt[idxs]
    cc_max = cc_max[idxs]
    event_pairs = event_pairs[idxs]


    return dict(
        dt=dt, 
        cc_max=cc_max,
        event_pairs=event_pairs,
        sorted_pair_ids=selected_pair_ids
    )



def correlate_wfs(X_template, X_test):
    
    xp = get_array_module(X_template)

    X_template = normalize(X_template, axis=0)
    X_test = normalize(X_test, axis=1)
    
    n_samp = X_template.shape[0]

    xcorr = xp.dstack([
        spx.ndimage.correlate1d(
            X_test[..., i_chan], 
            X_template[..., i_chan],
            mode='constant'
        )
        for i_chan in range(3)
    ])
    X_test = sliding_window(X_test, n_samp, axis=1)
    norm = (
        xp.sqrt(xp.sum(xp.square(X_template), axis=0))
        * 
        xp.sqrt(xp.sum(xp.square(X_test), axis=2))
    )
    
    return xp.nan_to_num(xcorr / norm)

    
def normalize(X, axis=0):
    
    xp = get_array_module(X)
    mu = xp.mean(X, axis=axis)
    inds = [slice(None) for i in range(X.ndim)]
    inds[axis] = xp.newaxis
    inds = tuple(inds)
    X = X - mu[inds]
    sigma = xp.std(X, axis=axis)
    X = X / sigma[inds]
    X = xp.nan_to_num(X)
    
    return X

    
def select_events(event, sorted_events, n_pairs):
    
    # Retain only the n_pairs nearest neighbours.
    selected_events = sorted_events.iloc[:n_pairs]

    # Remember the event pair IDs that were just correlated.
    selected_pairs = form_pairs(event.name, selected_events.index.values)
    
    return selected_events, selected_pairs


def sliding_window(X, window_length, axis=0):
    
    xp = get_array_module(X)
    n_pad = window_length // 2
    odd_bit = window_length % 2
    pad = [(0, 0)] * X.ndim
    pad[axis] = (n_pad+odd_bit, n_pad)
    X = xp.pad(X, pad)
    n_windows = X.shape[axis] - window_length
    shape = (*X.shape[:axis], n_windows, window_length, *X.shape[axis+1:])
    strides = (
        *X.strides[:axis],
         X.strides[axis],
        *X.strides[axis:]
    )
    X_new = xp.lib.stride_tricks.as_strided(
        X.copy(),
        shape=shape,
        strides=strides,
    )

    return X_new


def dist_sort(events, event, max_dist=None):
    
    geod = pyproj.Geod(ellps='WGS84')
    az, baz, dist = geod.inv(
        np.repeat(event['longitude'], len(events)),
        np.repeat(event['latitude'], len(events)),
        events['longitude'],
        events['latitude']
    )
    events['dist'] = np.sqrt(
        np.square(dist * 1e-3)
        +np.square(events['depth'] - event['depth'])
    )

    if max_dist is not None:
        events = events[events['dist'] < max_dist]

    events = events.sort_values('dist')
    
    return events


def form_pairs(template_event_id, test_event_ids, sort=False):
    # Form a list of event pair IDs for the sorted events.
    return pd.Series(
        tuple(zip(
            np.repeat(template_event_id, len(test_event_ids)), 
            test_event_ids
        )), 
        dtype=object
    )


def select_events_with_waveforms(events, wf_file, phase):
    event_ids = wf_file['event_id'][:].astype(str)
    array_index_map = pd.Series(
        np.arange(len(event_ids)),
        index=event_ids
    )
    mask = wf_file[f'mask_{phase}'][:]
    event_ids = event_ids[mask]
    # Just events with waveforms
    events = events.loc[event_ids]
    # Assign lookup index.
    events['array_index'] = array_index_map[event_ids]
    
    return events


def get_waveforms(wf_file, phase, template_event, test_events, config):
    n_samp_lead = wf_file[phase].attrs['n_samp_lead']
    n_samp_lag = wf_file[phase].attrs['n_samp_lag']
    sampling_rate = wf_file[phase].attrs['sampling_rate']
    
    template_tlead = config['waveforms'][f'template_tlead_{phase}']
    template_tlag  = config['waveforms'][f'template_tlag_{phase}']
    template_tlead = config['waveforms'][f'template_tlead_{phase}']
    template_n_samp_lead = int(sampling_rate*template_tlead)
    template_n_samp_lag = int(sampling_rate*template_tlag)
    
    i_samp_start = n_samp_lead - template_n_samp_lead
    i_samp_end   = n_samp_lead + template_n_samp_lag
    if i_samp_start < 0 or i_samp_end > n_samp_lead+n_samp_lag:
        logger.warning('Event waveforms are too short for request template length')
        i_samp_start = max(0, i_samp_start)
        i_samp_end = min(n_samp_lead+n_samp_lag, i_samp_end)
    
    template_event_idx = template_event['array_index']
    test_event_idxs = test_events['array_index'].values
    
    template_wf = wf_file[phase][template_event_idx, i_samp_start: i_samp_end]
    template_start_time = wf_file[f'start_time_{phase}'][template_event_idx]
    template_start_time += i_samp_start / sampling_rate

    test_wfs = wf_file[phase][test_event_idxs]
    test_start_times = wf_file[f'start_time_{phase}'][test_event_idxs]
    
    return template_wf, test_wfs, template_start_time, test_start_times


def initialize_output(config, clargs, event_ids):
    out_file = config['general']['output_dir'].joinpath(
        f'{clargs.network}.{clargs.station}.hdf5'
    )
    out_file.parent.mkdir(exist_ok=True, parents=True)
    n_events = len(event_ids)
    with h5py.File(out_file, mode='a') as out_file:
        out_file.create_dataset(
            '/event_ids',
            data=event_ids,
            dtype=h5py.string_dtype()
        )
        for phase in clargs.phases:
            out_file.create_dataset(
                f'/event_pairs/{phase}',
                (0, 2),
                maxshape=(None, 2),
                dtype=h5py.string_dtype()
            )
            out_file.create_dataset(
                f'/dt/{phase}',
                (n_events, n_events),
                maxshape=(None, None),
                dtype=np.float64,
                chunks=(32, 32),
                compression=3
            )
            out_file.create_dataset(
                f'/cc_max/{phase}',
                (n_events, n_events),
                maxshape=(None, None),
                dtype=np.float32,
                chunks=(32, 32),
                compression=3
            )
        file_name = out_file.filename

    return file_name
    

def load_events(config):
    events = pd.read_hdf(config['general']['catalog'], key='/events')
    events = events.set_index('event_id')
    events = events.sort_index()

    return events


def log_clargs(clargs):
    name = pathlib.Path(sys.argv[0]).name
    logger.info(f'Starting script {name}...')
    logger.info('')
    logger.info('***Command-line arguments***')
    for arg in sorted(vars(clargs)):
        logger.info(f'{arg}: {getattr(clargs, arg)}')
    logger.info('')
    
    
def log_config(config):
    logger.info('')
    logger.info('***Configuration***')
    for group in sorted(config):
        if not isinstance(config[group], dict):
            value = config[group]
            logger.info(f'{group}: {value}')
            continue
        for key in sorted(config[group]):
            value = config[group][key]
            logger.info(f'{group}.{key}: {value}')
    logger.info('')

    
if __name__ == '__main__':
    main()
